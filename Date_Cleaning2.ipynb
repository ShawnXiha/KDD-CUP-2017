{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 Data_Cleaning.ipynb的基础上做的改变\n",
    "1. 将72个20分钟段特征去掉,改成24个小时段+一个小时的3个20分钟段组合\n",
    "2. 增加天气数据\n",
    " - 天气中风向的异常值99107用他前一个风向与后一个风向的平均值代替"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 读取测试训练的t3,t4,t5,t6.t7,合并训练测试的t7生成天气数据\n",
    "2. 按照20分钟时间窗口得到 t5_train,t6_train,t5_test_2h_ago,t6_testt_2h_ago\n",
    "3. t5_test_2h_ago,t6_testt_2h_ago,时间窗口加2小时去掉标签,得到t5_test,t6_test\n",
    "4. 按照时间日期给t5_train,t6_train,t5_test,t6_test,加特征是否假期,星期几,几点钟,在那个20分钟段\n",
    "5. 按照时间日期增加天气数据并fillna\n",
    "6. 清洗t4,t5得每条路线的,宽度长度岔路数,t5_train,t5_test按照Intersection 和 Tollgates 添加路途宽度长度岔路数特征\n",
    "7. t5_train,t6_train,t5_test,t6_test数据格式修改到要求的格式,保存到csv文件中\n",
    "8. 查看文件发现并修改一些问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob\n",
    "t3 = pd.read_csv('dataSets/training/links (table 3).csv')\n",
    "t4 = pd.read_csv('dataSets/training/routes (table 4).csv')\n",
    "t5_train = pd.read_csv('dataSets/training/trajectories(table 5)_training.csv')\n",
    "t6_train = pd.read_csv(glob.glob(\"dataSets/training/*.csv\")[-1])\n",
    "t6_test_2h_ago = pd.read_csv(glob.glob(\"dataSets/testing_phase1/*.csv\")[0])\n",
    "t5_test_2h_ago = pd.read_csv(glob.glob(\"dataSets/testing_phase1/*.csv\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "t7 = pd.read_csv(glob.glob(\"dataSets/training/*.csv\")[0])\n",
    "t71 =pd.read_csv(glob.glob(\"dataSets/testing_phase1/*.csv\")[1])\n",
    "t7 = t7.append(t71).reset_index()\n",
    "del t71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 修改风向\n",
    "for i, row in t7.iterrows():\n",
    "    if row['wind_direction']== 999017.0:\n",
    "        last = t7.loc[i-1,'wind_direction']\n",
    "        nest = t7.loc[i+1,'wind_direction']\n",
    "        t7.loc[i, 'wind_direction'] = (last + nest)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t7.wind_direction.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 按照时间窗口重组数据: http://stackoverflow.com/questions/36914892/python-how-to-group-pandas-data-frame-in-a-certain-time-window\n",
    "t5_train['starting_time'] =  pd.to_datetime(t5_train['starting_time'] , format='%Y-%m-%d %H:%M:%S')\n",
    "t5_train = t5_train.set_index(['starting_time'])\n",
    "\n",
    "t5_train = t5_train.groupby([pd.TimeGrouper('20Min'), 'intersection_id', 'tollgate_id']).travel_time.mean()\\\n",
    "       .reset_index().rename(columns = {'travel_time':'avg_travel_time'})\n",
    "    \n",
    "t5_test_2h_ago['starting_time'] =  pd.to_datetime(t5_test_2h_ago['starting_time'] , format='%Y-%m-%d %H:%M:%S')\n",
    "t5_test_2h_ago = t5_test_2h_ago.set_index(['starting_time'])\n",
    "\n",
    "t5_test_2h_ago = t5_test_2h_ago.groupby([pd.TimeGrouper('20Min'), 'intersection_id', 'tollgate_id']).travel_time.mean()\\\n",
    "       .reset_index().rename(columns = {'travel_time':'avg_travel_time'})\n",
    "    \n",
    "    \n",
    "t6_train['time'] =  pd.to_datetime(t6_train['time'] , format='%Y-%m-%d %H:%M:%S')\n",
    "t6_train = t6_train.set_index(['time'])\n",
    "\n",
    "# 车流量\n",
    "t6_train = t6_train.groupby([pd.TimeGrouper('20Min'), 'tollgate_id', 'direction']).size()\\\n",
    "       .reset_index().rename(columns = {0:'volume'})\n",
    "    \n",
    "    \n",
    "t6_test_2h_ago['time'] =  pd.to_datetime(t6_test_2h_ago['time'] , format='%Y-%m-%d %H:%M:%S')\n",
    "t6_test_2h_ago = t6_test_2h_ago.set_index(['time'])\n",
    "\n",
    "# 车流量\n",
    "t6_test_2h_ago = t6_test_2h_ago.groupby([pd.TimeGrouper('20Min'), 'tollgate_id', 'direction']).size()\\\n",
    "       .reset_index().rename(columns = {0:'volume'})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apped 2h前测试数据\n",
    "t5_train= t5_train.append(t5_test_2h_ago).reset_index()\n",
    "t6_train= t6_train.append(t6_test_2h_ago).reset_index()\n",
    "# 增加两小时作为测试数据\n",
    "t5_test_2h_ago['starting_time'] = t5_test_2h_ago['starting_time'] + pd.DateOffset(hours=2)\n",
    "t6_test_2h_ago['time'] = t6_test_2h_ago['time'] + pd.DateOffset(hours=2)\n",
    "# 测试数据\n",
    "t5_test,t6_test =t5_test_2h_ago,t6_test_2h_ago\n",
    "del t5_test_2h_ago,t6_test_2h_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 中秋国庆假期\n",
    "\n",
    "from datetime import datetime\n",
    "start1 = datetime(2016, 9, 15)\n",
    "end1 = datetime(2016, 9, 17)\n",
    "start2 = datetime(2016, 10, 1)\n",
    "end2 = datetime(2016, 10, 7)\n",
    "rng = pd.date_range(start1, end1).append(pd.date_range(start2, end2))\n",
    "\n",
    "# 是否假期,星期,小时,在那个时间窗口\n",
    "\n",
    "def generateDataTime(X,label):\n",
    "    \n",
    "    # 小时,星期,时间窗口\n",
    "    hour = pd.get_dummies(X[label].dt.hour,prefix='hour_')\n",
    "    weekday = pd.get_dummies(X[label].dt.weekday_name)\n",
    "    minute= pd.get_dummies(X[label].dt.minute)\n",
    "    # 添加到X\n",
    "    X = pd.concat([X,weekday,hour, minute], axis=1)\n",
    "    # 增加 date hour特征方便与t7配对以添加天气数据\n",
    "    X['date']=X[label].dt.date\n",
    "    X['date'] = X['date'].astype(str)\n",
    "    X['date'] = pd.to_datetime(X['date'], format='%Y-%m-%d')\n",
    "    X['hour']=X[label].dt.hour.astype(int)\n",
    "    # 是否在假期内\n",
    "    date = X[label].dt.date\n",
    "    for i, row in X.iterrows():\n",
    "        X.loc[i, \"holiday\"] = 0\n",
    "        if date.loc[i] in rng: X.loc[i, \"holiday\"] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成包含日期数据的数据\n",
    "t5_test = generateDataTime(t5_test,'starting_time')\n",
    "t6_test = generateDataTime(t6_test,'time')\n",
    "t5_train = generateDataTime(t5_train,'starting_time')\n",
    "t6_train = generateDataTime(t6_train,'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按照日期时间添加天气数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t7['date'] = pd.to_datetime(t7['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将hour转变成3小时间隔,然后与天气数据组合\n",
    "def add_weather(df):\n",
    "    for i, row in df.iterrows():\n",
    "        if row['hour'] in [23,0,1]: df.loc[i, \"hour\"] = 0\n",
    "        elif row['hour'] in [2,3,4]: df.loc[i, \"hour\"] = 3 \n",
    "        elif row['hour'] in [5,6,7]: df.loc[i, \"hour\"] = 6         \n",
    "        elif row['hour'] in [8,9,10]: df.loc[i, \"hour\"] = 9         \n",
    "        elif row['hour'] in [11,12,13]: df.loc[i, \"hour\"] = 12         \n",
    "        elif row['hour'] in [14,15,16]: df.loc[i, \"hour\"] = 15         \n",
    "        elif row['hour'] in [17,18,19]: df.loc[i, \"hour\"] = 18         \n",
    "        elif row['hour'] in [20,21,22]: df.loc[i, \"hour\"] = 21\n",
    "    return pd.merge(df,t7,on =['date', 'hour'] ,how='left')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t5_test = add_weather(t5_test)\n",
    "t6_test = add_weather(t6_test)\n",
    "t5_train =add_weather(t5_train)\n",
    "t6_train = add_weather(t6_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 删除不必要的列\n",
    "t5_test = t5_test.drop(['hour','date'],axis=1)\n",
    "t6_test = t6_test.drop(['hour','date'],axis=1)\n",
    "t5_train =t5_train.drop(['index_x','hour','date','index_y'],axis=1)\n",
    "t6_train = t6_train.drop(['index_x','hour','date','index_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 道路信息聚合\n",
    "\n",
    "# 将110,123 变成[110,123] \n",
    "def split(str): return str.split(\",\")\n",
    "t4.link_seq = t4.link_seq.apply(split)\n",
    "\n",
    "# 将数据按link_seq中各link展开\n",
    "rows = []\n",
    "_ = t4.apply(lambda row: [rows.append([row['intersection_id'], row['tollgate_id'], nn]) \n",
    "                         for nn in row.link_seq], axis=1)\n",
    "col = ['intersection_id', 'tollgate_id','link_id']\n",
    "t4_new = pd.DataFrame(rows, columns=col)\n",
    "\n",
    "# pd.merge(t4_new,t3,on = 'link_id',how='left')\n",
    "# 提取每条link的是否有in,out交叉路口\n",
    "t3['cross_in'] = 0\n",
    "t3['cross_out'] = 0\n",
    "\n",
    "for i, row in t3.iterrows():\n",
    "    if ',' in str(row['in_top']):\n",
    "        t3.loc[i, \"cross_in\"] = 1\n",
    "    if ',' in str(row['out_top']):\n",
    "        t3.loc[i, \"cross_out\"] = 1\n",
    "        \n",
    "t3['link_id'] = t3['link_id'].astype(str)\n",
    "t4_new['link_id'] = t4_new['link_id'].astype(str)\n",
    "t4_new = pd.merge(t4_new,t3,on = 'link_id',how='left')\n",
    "t4_new.drop(['in_top','out_top'],axis =1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 聚合t4_new\n",
    "# in out 岔路口数\n",
    "b1 = t4_new[['intersection_id', 'tollgate_id','cross_in']]\\\n",
    "           .groupby(['intersection_id', 'tollgate_id'])\\\n",
    "            .cross_in.sum().reset_index().rename(columns = {'cross_in':'in_link_cross_count'})\n",
    "b2 = t4_new[['intersection_id', 'tollgate_id','cross_out']]\\\n",
    "        .groupby(['intersection_id', 'tollgate_id'])\\\n",
    "        .cross_out.sum().reset_index().rename(columns = {'cross_out':'out_link_cross_count'})\n",
    "road = pd.merge(b1,b2,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "\n",
    "# 路程\n",
    "b1 = t4_new[['intersection_id', 'tollgate_id','length']].groupby(['intersection_id', 'tollgate_id']).length.sum().reset_index()\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "## 车道数1车道2车道3车道4车道的link数，后期考虑：占总路程的比率\n",
    "# 各个路径的link总数\n",
    "b1 = t4_new[['intersection_id', 'tollgate_id']]\\\n",
    "       .groupby(['intersection_id', 'tollgate_id']).size()\\\n",
    "    .reset_index().rename(columns = {0:'link_count'})\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1,2,3,4车道道路长度\n",
    "b1 = t4_new[t4_new.lanes== 1][['intersection_id', 'tollgate_id','length']]\\\n",
    "       .groupby(['intersection_id', 'tollgate_id']).length.sum()\\\n",
    "    .reset_index().rename(columns = {'length':'1_length'})\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "# 2车道路长度\n",
    "b1 = t4_new[t4_new.lanes== 2][['intersection_id', 'tollgate_id','length']]\\\n",
    "       .groupby(['intersection_id', 'tollgate_id']).length.sum()\\\n",
    "    .reset_index().rename(columns = {'length':'2_length'})\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "# 3车道路长度\n",
    "b1 = t4_new[t4_new.lanes== 3][['intersection_id', 'tollgate_id','length']]\\\n",
    "       .groupby(['intersection_id', 'tollgate_id']).length.sum()\\\n",
    "    .reset_index().rename(columns = {'length':'3_length'})\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "# 4车道路长度\n",
    "b1 = t4_new[t4_new.lanes== 4][['intersection_id', 'tollgate_id','length']]\\\n",
    "       .groupby(['intersection_id', 'tollgate_id']).length.sum()\\\n",
    "    .reset_index().rename(columns = {'length':'4_length'})\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = t4_new[t4_new.lanes== 1][['intersection_id', 'tollgate_id']]\\\n",
    "       .groupby(['intersection_id', 'tollgate_id']).size()\\\n",
    "    .reset_index().rename(columns = {0:'1_count'})\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "# 2车道link数\n",
    "b1 = t4_new[t4_new.lanes== 2][['intersection_id', 'tollgate_id']]\\\n",
    "       .groupby(['intersection_id', 'tollgate_id']).size()\\\n",
    "    .reset_index().rename(columns = {0:'2_count'})\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "# 3车道link数\n",
    "b1 = t4_new[t4_new.lanes== 3][['intersection_id', 'tollgate_id']]\\\n",
    "       .groupby(['intersection_id', 'tollgate_id']).size()\\\n",
    "    .reset_index().rename(columns = {0:'3_count'})\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "# 4车道link数\n",
    "b1 = t4_new[t4_new.lanes== 4][['intersection_id', 'tollgate_id']]\\\n",
    "       .groupby(['intersection_id', 'tollgate_id']).size()\\\n",
    "    .reset_index().rename(columns = {0:'4_count'})\n",
    "road = pd.merge(road,b1,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "\n",
    "road.fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t5_test = pd.merge(t5_test,road,on =['intersection_id', 'tollgate_id'] ,how='left')\n",
    "t5_train = pd.merge(t5_train,road,on =['intersection_id', 'tollgate_id'] ,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t5_test['end'] = t5_test['starting_time'] + pd.DateOffset(minutes=20)\n",
    "t5_train['end'] = t5_train['starting_time'] + pd.DateOffset(minutes=20)\n",
    "t6_test['end'] = t6_test['time'] + pd.DateOffset(minutes=20)\n",
    "t6_train['end'] = t6_train['time'] + pd.DateOffset(minutes=20)\n",
    "\n",
    "\n",
    "# http://stackoverflow.com/questions/30132282/datetime-to-string-with-series-in-python-pandas\n",
    "def timewindows(X,start,end):\n",
    "    s= X[start].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    e = X[end].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    X['time_window'] = '['+ s +','+ e +')'\n",
    "    return X.drop([start,end],axis =1)\n",
    "\n",
    "\n",
    "t5_test = timewindows(t5_test,'starting_time','end')\n",
    "t5_train = timewindows(t5_train,'starting_time','end')\n",
    "t6_test = timewindows(t6_test,'time','end')\n",
    "t6_train = timewindows(t6_train,'time','end')\n",
    "\n",
    "t5_test = t5_test.set_index(['intersection_id','tollgate_id','time_window'])\n",
    "t5_train = t5_train.set_index(['intersection_id','tollgate_id','time_window'])\n",
    "t6_test = t6_test.set_index(['tollgate_id','time_window', 'direction'])\n",
    "t6_train = t6_train.set_index(['tollgate_id','time_window', 'direction'])\n",
    "\n",
    "t5_testcol,t5_traincol = list(t5_test.columns.values),list(t5_train.columns.values)\n",
    "t6_testcol,t6_traincol = list(t6_test.columns.values),list(t6_train.columns.values)\n",
    "mis_5 =  [x for x in t5_traincol  if x not in t5_testcol]\n",
    "mis_6 =  [x for x in t6_traincol  if x not in t6_testcol]\n",
    "\n",
    "for label in mis_5:\n",
    "    t5_test[label] = 0\n",
    "for label in mis_6:\n",
    "    t6_test[label] = 0\n",
    "    \n",
    "# 调整各列顺序\n",
    "t5_test = t5_test[t5_traincol]\n",
    "t6_test = t6_test[t6_traincol]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill(df):\n",
    "    return df.fillna(df.mean())\n",
    "t5_test = fill(t5_test)\n",
    "t6_test = fill(t6_test)\n",
    "t5_train =fill(t5_train)\n",
    "t6_train =fill(t6_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t5_test.to_csv('dataSets/big_testset_task1_version2.csv')\n",
    "t5_train.to_csv('dataSets/big_trainset_task1_version2.csv')\n",
    "t6_test.to_csv('dataSets/big_testset_task2_version2.csv')\n",
    "t6_train.to_csv('dataSets/big_trainset_task2_version2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
